{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DMD2 Distillation Training - MNIST\n",
        "\n",
        "This notebook trains a fast feedforward model using DMD2 distillation on MNIST.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision tqdm -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import copy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Definitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model definition (same as teacher training)\n",
        "def get_sigmas_karras(n, sigma_min=0.002, sigma_max=80.0, rho=7.0):\n",
        "    \"\"\"Generate Karras noise schedule\"\"\"\n",
        "    ramp = torch.linspace(0, 1, n)\n",
        "    min_inv_rho = sigma_min ** (1 / rho)\n",
        "    max_inv_rho = sigma_max ** (1 / rho)\n",
        "    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\n",
        "    return sigmas\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    \"\"\"Sinusoidal time embedding\"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = np.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = time[:, None] * emb[None, :]\n",
        "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"Residual block with time conditioning\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim, out_channels)\n",
        "        )\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.GroupNorm(8, in_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.GroupNorm(8, out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        )\n",
        "        self.res_conv = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        h = self.block1(x)\n",
        "        time_emb = self.time_mlp(time_emb)\n",
        "        h = h + time_emb[:, :, None, None]\n",
        "        h = self.block2(h)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"Self-attention block\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.norm = nn.GroupNorm(8, channels)\n",
        "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
        "        self.proj = nn.Conv2d(channels, channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.norm(x)\n",
        "        qkv = self.qkv(h)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        \n",
        "        # Reshape for attention\n",
        "        q = q.view(B, C, H * W).permute(0, 2, 1)\n",
        "        k = k.view(B, C, H * W)\n",
        "        v = v.view(B, C, H * W).permute(0, 2, 1)\n",
        "        \n",
        "        # Attention\n",
        "        scale = (C // 1) ** -0.5\n",
        "        attn = torch.softmax(q @ k * scale, dim=-1)\n",
        "        h = (attn @ v).permute(0, 2, 1).view(B, C, H, W)\n",
        "        \n",
        "        return x + self.proj(h)\n",
        "\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    \"\"\"Simple UNet for MNIST\"\"\"\n",
        "    def __init__(self, img_channels=1, label_dim=10, time_emb_dim=128):\n",
        "        super().__init__()\n",
        "        self.time_emb_dim = time_emb_dim\n",
        "        self.time_embed = TimeEmbedding(time_emb_dim)\n",
        "        \n",
        "        # Label embedding\n",
        "        self.label_embed = nn.Embedding(label_dim, time_emb_dim)\n",
        "        \n",
        "        # Downsampling\n",
        "        self.conv_in = nn.Conv2d(img_channels, 64, 3, padding=1)\n",
        "        self.down1 = ResBlock(64, 128, time_emb_dim)\n",
        "        self.down2 = ResBlock(128, 256, time_emb_dim)\n",
        "        self.down3 = ResBlock(256, 512, time_emb_dim)\n",
        "        \n",
        "        # Middle\n",
        "        self.mid_block1 = ResBlock(512, 512, time_emb_dim)\n",
        "        self.mid_attn = AttentionBlock(512)\n",
        "        self.mid_block2 = ResBlock(512, 512, time_emb_dim)\n",
        "        \n",
        "        # Upsampling\n",
        "        self.up1 = ResBlock(512 + 256, 256, time_emb_dim)\n",
        "        self.up2 = ResBlock(256 + 128, 128, time_emb_dim)\n",
        "        self.up3 = ResBlock(128 + 64, 64, time_emb_dim)\n",
        "        \n",
        "        # Output\n",
        "        self.norm_out = nn.GroupNorm(8, 64)\n",
        "        self.conv_out = nn.Conv2d(64, img_channels, 3, padding=1)\n",
        "        \n",
        "    def forward(self, x, sigma, label, return_bottleneck=False):\n",
        "        # Handle sigma\n",
        "        if isinstance(sigma, (int, float)) or (isinstance(sigma, torch.Tensor) and sigma.dim() == 0):\n",
        "            sigma = torch.full((x.shape[0],), float(sigma), device=x.device)\n",
        "        elif sigma.dim() > 1:\n",
        "            sigma = sigma.squeeze()\n",
        "        \n",
        "        # Handle label\n",
        "        if label.dim() > 1:\n",
        "            label = label.argmax(dim=1)\n",
        "        \n",
        "        # Time embedding from sigma\n",
        "        time_emb = self.time_embed(sigma)\n",
        "        label_emb = self.label_embed(label)\n",
        "        time_emb = time_emb + label_emb\n",
        "        \n",
        "        # Downsampling\n",
        "        h1 = self.conv_in(x)\n",
        "        h2 = self.down1(h1, time_emb)\n",
        "        h2_down = nn.functional.avg_pool2d(h2, 2)\n",
        "        h3 = self.down2(h2_down, time_emb)\n",
        "        h3_down = nn.functional.avg_pool2d(h3, 2)\n",
        "        h4 = self.down3(h3_down, time_emb)\n",
        "        h4_down = nn.functional.avg_pool2d(h4, 2)\n",
        "        \n",
        "        # Middle\n",
        "        h = self.mid_block1(h4_down, time_emb)\n",
        "        h = self.mid_attn(h)\n",
        "        h = self.mid_block2(h, time_emb)\n",
        "        \n",
        "        if return_bottleneck:\n",
        "            return h\n",
        "        \n",
        "        # Upsampling\n",
        "        h = nn.functional.interpolate(h, size=h3.shape[2:], mode='nearest')\n",
        "        h = torch.cat([h, h3], dim=1)\n",
        "        h = self.up1(h, time_emb)\n",
        "        \n",
        "        h = nn.functional.interpolate(h, size=h2.shape[2:], mode='nearest')\n",
        "        h = torch.cat([h, h2], dim=1)\n",
        "        h = self.up2(h, time_emb)\n",
        "        \n",
        "        h = nn.functional.interpolate(h, size=h1.shape[2:], mode='nearest')\n",
        "        h = torch.cat([h, h1], dim=1)\n",
        "        h = self.up3(h, time_emb)\n",
        "        \n",
        "        # Output\n",
        "        h = self.norm_out(h)\n",
        "        h = nn.functional.silu(h)\n",
        "        out = self.conv_out(h)\n",
        "        \n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guidance Model\n",
        "class GuidanceModel(nn.Module):\n",
        "    \"\"\"Guidance model for DMD2 training\"\"\"\n",
        "    def __init__(self, num_train_timesteps=1000, sigma_min=0.002, sigma_max=80.0, \n",
        "                 sigma_data=0.5, rho=7.0, min_step_percent=0.02, max_step_percent=0.98):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Real UNet (teacher) - frozen\n",
        "        self.real_unet = SimpleUNet(img_channels=1, label_dim=10)\n",
        "        self.real_unet.requires_grad_(False)\n",
        "        \n",
        "        # Fake UNet (student) - trainable\n",
        "        self.fake_unet = copy.deepcopy(self.real_unet)\n",
        "        self.fake_unet.requires_grad_(True)\n",
        "        \n",
        "        # Training parameters\n",
        "        self.sigma_data = sigma_data\n",
        "        self.sigma_max = sigma_max\n",
        "        self.sigma_min = sigma_min\n",
        "        self.rho = rho\n",
        "        self.num_train_timesteps = num_train_timesteps\n",
        "        \n",
        "        # Karras noise schedule\n",
        "        karras_sigmas = torch.flip(\n",
        "            get_sigmas_karras(num_train_timesteps, sigma_max=sigma_max, \n",
        "                             sigma_min=sigma_min, rho=rho),\n",
        "            dims=[0]\n",
        "        )\n",
        "        self.register_buffer(\"karras_sigmas\", karras_sigmas)\n",
        "        \n",
        "        self.min_step = int(min_step_percent * num_train_timesteps)\n",
        "        self.max_step = int(max_step_percent * num_train_timesteps)\n",
        "    \n",
        "    def compute_distribution_matching_loss(self, latents, labels):\n",
        "        batch_size = latents.shape[0]\n",
        "        \n",
        "        # Sample random timesteps\n",
        "        with torch.no_grad():\n",
        "            timesteps = torch.randint(\n",
        "                self.min_step,\n",
        "                min(self.max_step + 1, self.num_train_timesteps),\n",
        "                [batch_size, 1, 1, 1],\n",
        "                device=latents.device,\n",
        "                dtype=torch.long\n",
        "            )\n",
        "            \n",
        "            noise = torch.randn_like(latents)\n",
        "            timestep_sigma = self.karras_sigmas[timesteps.squeeze()]\n",
        "            \n",
        "            # Add noise\n",
        "            noisy_latents = latents + timestep_sigma.reshape(-1, 1, 1, 1) * noise\n",
        "            \n",
        "            # Predictions from both models\n",
        "            pred_real_image = self.real_unet(noisy_latents, timestep_sigma, labels)\n",
        "            pred_fake_image = self.fake_unet(noisy_latents, timestep_sigma, labels)\n",
        "            \n",
        "            # Compute gradient direction\n",
        "            p_real = latents - pred_real_image\n",
        "            p_fake = latents - pred_fake_image\n",
        "            \n",
        "            # Weight factor for normalization\n",
        "            weight_factor = torch.abs(p_real).mean(dim=[1, 2, 3], keepdim=True)\n",
        "            grad = (p_real - p_fake) / (weight_factor + 1e-8)\n",
        "            grad = torch.nan_to_num(grad)\n",
        "        \n",
        "        # Distribution matching loss (gradient matching)\n",
        "        loss = 0.5 * F.mse_loss(latents, (latents - grad).detach(), reduction=\"mean\")\n",
        "        \n",
        "        loss_dict = {\"loss_dm\": loss}\n",
        "        log_dict = {\n",
        "            \"dmtrain_noisy_latents\": noisy_latents.detach(),\n",
        "            \"dmtrain_pred_real_image\": pred_real_image.detach(),\n",
        "            \"dmtrain_pred_fake_image\": pred_fake_image.detach(),\n",
        "            \"dmtrain_grad\": grad.detach(),\n",
        "            \"dmtrain_gradient_norm\": torch.norm(grad).item(),\n",
        "            \"dmtrain_timesteps\": timesteps.detach(),\n",
        "        }\n",
        "        \n",
        "        return loss_dict, log_dict\n",
        "    \n",
        "    def compute_loss_fake(self, latents, labels):\n",
        "        batch_size = latents.shape[0]\n",
        "        latents = latents.detach()  # No gradient to generator\n",
        "        \n",
        "        noise = torch.randn_like(latents)\n",
        "        \n",
        "        # Sample random timesteps\n",
        "        timesteps = torch.randint(\n",
        "            0,\n",
        "            self.num_train_timesteps,\n",
        "            [batch_size, 1, 1, 1],\n",
        "            device=latents.device,\n",
        "            dtype=torch.long\n",
        "        )\n",
        "        timestep_sigma = self.karras_sigmas[timesteps.squeeze()]\n",
        "        \n",
        "        # Add noise\n",
        "        noisy_latents = latents + timestep_sigma.reshape(-1, 1, 1, 1) * noise\n",
        "        \n",
        "        # Predict x0\n",
        "        fake_x0_pred = self.fake_unet(noisy_latents, timestep_sigma, labels)\n",
        "        \n",
        "        # Karras weighting\n",
        "        snrs = timestep_sigma ** -2\n",
        "        weights = snrs + 1.0 / (self.sigma_data ** 2)\n",
        "        \n",
        "        target = latents\n",
        "        \n",
        "        loss_fake = torch.mean(weights.reshape(-1, 1, 1, 1) * (fake_x0_pred - target) ** 2)\n",
        "        \n",
        "        loss_dict = {\"loss_fake_mean\": loss_fake}\n",
        "        log_dict = {\n",
        "            \"faketrain_latents\": latents.detach(),\n",
        "            \"faketrain_noisy_latents\": noisy_latents.detach(),\n",
        "            \"faketrain_x0_pred\": fake_x0_pred.detach()\n",
        "        }\n",
        "        \n",
        "        return loss_dict, log_dict\n",
        "    \n",
        "    def forward(self, generator_turn=False, guidance_turn=False,\n",
        "                generator_data_dict=None, guidance_data_dict=None):\n",
        "        if generator_turn:\n",
        "            assert generator_data_dict is not None\n",
        "            loss_dict, log_dict = self.compute_distribution_matching_loss(\n",
        "                generator_data_dict['image'],\n",
        "                generator_data_dict['label']\n",
        "            )\n",
        "        elif guidance_turn:\n",
        "            assert guidance_data_dict is not None\n",
        "            loss_dict, log_dict = self.compute_loss_fake(\n",
        "                guidance_data_dict['image'],\n",
        "                guidance_data_dict['label']\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Either generator_turn or guidance_turn must be True\")\n",
        "        \n",
        "        return loss_dict, log_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unified Model\n",
        "class UnifiedModel(nn.Module):\n",
        "    \"\"\"Unified model wrapping generator and guidance\"\"\"\n",
        "    def __init__(self, num_train_timesteps=1000, sigma_min=0.002, sigma_max=80.0,\n",
        "                 sigma_data=0.5, rho=7.0, min_step_percent=0.02, max_step_percent=0.98,\n",
        "                 conditioning_sigma=80.0):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Guidance model (contains real_unet and fake_unet)\n",
        "        self.guidance_model = GuidanceModel(\n",
        "            num_train_timesteps=num_train_timesteps,\n",
        "            sigma_min=sigma_min,\n",
        "            sigma_max=sigma_max,\n",
        "            sigma_data=sigma_data,\n",
        "            rho=rho,\n",
        "            min_step_percent=min_step_percent,\n",
        "            max_step_percent=max_step_percent\n",
        "        )\n",
        "        \n",
        "        # Feedforward generator model (initialized from fake_unet)\n",
        "        self.feedforward_model = copy.deepcopy(self.guidance_model.fake_unet)\n",
        "        self.feedforward_model.requires_grad_(True)\n",
        "        \n",
        "        self.conditioning_sigma = conditioning_sigma\n",
        "        self.num_train_timesteps = num_train_timesteps\n",
        "    \n",
        "    def forward(self, scaled_noisy_image, timestep_sigma, labels,\n",
        "                real_train_dict=None,\n",
        "                compute_generator_gradient=False,\n",
        "                generator_turn=False,\n",
        "                guidance_turn=False,\n",
        "                guidance_data_dict=None):\n",
        "        assert (generator_turn and not guidance_turn) or (guidance_turn and not generator_turn)\n",
        "        \n",
        "        if generator_turn:\n",
        "            # Generate image with feedforward model\n",
        "            if not compute_generator_gradient:\n",
        "                with torch.no_grad():\n",
        "                    generated_image = self.feedforward_model(\n",
        "                        scaled_noisy_image, timestep_sigma, labels\n",
        "                    )\n",
        "            else:\n",
        "                generated_image = self.feedforward_model(\n",
        "                    scaled_noisy_image, timestep_sigma, labels\n",
        "                )\n",
        "            \n",
        "            # Compute distribution matching loss if needed\n",
        "            if compute_generator_gradient:\n",
        "                generator_data_dict = {\n",
        "                    \"image\": generated_image,\n",
        "                    \"label\": labels,\n",
        "                    \"real_train_dict\": real_train_dict\n",
        "                }\n",
        "                \n",
        "                # Disable gradient for guidance model to avoid side effects\n",
        "                self.guidance_model.requires_grad_(False)\n",
        "                loss_dict, log_dict = self.guidance_model(\n",
        "                    generator_turn=True,\n",
        "                    guidance_turn=False,\n",
        "                    generator_data_dict=generator_data_dict\n",
        "                )\n",
        "                self.guidance_model.requires_grad_(True)\n",
        "            else:\n",
        "                loss_dict = {}\n",
        "                log_dict = {}\n",
        "            \n",
        "            log_dict['generated_image'] = generated_image.detach()\n",
        "            log_dict['guidance_data_dict'] = {\n",
        "                \"image\": generated_image.detach(),\n",
        "                \"label\": labels.detach(),\n",
        "                \"real_train_dict\": real_train_dict\n",
        "            }\n",
        "        \n",
        "        elif guidance_turn:\n",
        "            assert guidance_data_dict is not None\n",
        "            loss_dict, log_dict = self.guidance_model(\n",
        "                generator_turn=False,\n",
        "                guidance_turn=True,\n",
        "                guidance_data_dict=guidance_data_dict\n",
        "            )\n",
        "        \n",
        "        return loss_dict, log_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "config = {\n",
        "    'data_dir': './data',\n",
        "    'output_dir': './checkpoints/dmd2',\n",
        "    'teacher_checkpoint': './checkpoints/teacher/teacher_final.pt',  # Update this path\n",
        "    'batch_size': 128,\n",
        "    'generator_lr': 2e-6,\n",
        "    'guidance_lr': 2e-6,\n",
        "    'num_epochs': 50,\n",
        "    'num_train_timesteps': 1000,\n",
        "    'sigma_min': 0.002,\n",
        "    'sigma_max': 80.0,\n",
        "    'sigma_data': 0.5,\n",
        "    'rho': 7.0,\n",
        "    'min_step_percent': 0.02,\n",
        "    'max_step_percent': 0.98,\n",
        "    'conditioning_sigma': 80.0,\n",
        "    'dfake_gen_update_ratio': 10,\n",
        "    'dm_loss_weight': 1.0,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'save_every': 5000,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(config['output_dir'], exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load MNIST Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=config['data_dir'],\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Dataset loaded: {len(train_dataset)} training samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize unified model\n",
        "model = UnifiedModel(\n",
        "    num_train_timesteps=config['num_train_timesteps'],\n",
        "    sigma_min=config['sigma_min'],\n",
        "    sigma_max=config['sigma_max'],\n",
        "    sigma_data=config['sigma_data'],\n",
        "    rho=config['rho'],\n",
        "    min_step_percent=config['min_step_percent'],\n",
        "    max_step_percent=config['max_step_percent'],\n",
        "    conditioning_sigma=config['conditioning_sigma']\n",
        ").to(device)\n",
        "\n",
        "# Load teacher checkpoint into real_unet\n",
        "if config['teacher_checkpoint']:\n",
        "    print(f\"Loading teacher checkpoint from {config['teacher_checkpoint']}\")\n",
        "    checkpoint = torch.load(config['teacher_checkpoint'], map_location=device)\n",
        "    model.guidance_model.real_unet.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"Teacher model loaded successfully\")\n",
        "\n",
        "# Optimizers\n",
        "optimizer_generator = optim.AdamW(\n",
        "    model.feedforward_model.parameters(),\n",
        "    lr=config['generator_lr'],\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "optimizer_guidance = optim.AdamW(\n",
        "    model.guidance_model.fake_unet.parameters(),\n",
        "    lr=config['guidance_lr'],\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Eye matrix for one-hot encoding\n",
        "eye_matrix = torch.eye(10, device=device)\n",
        "\n",
        "print(f\"Model initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "model.train()\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(config['num_epochs']):\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
        "    epoch_loss_dm = 0.0\n",
        "    epoch_loss_fake = 0.0\n",
        "    \n",
        "    for batch_idx, (images, labels) in enumerate(pbar):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Convert labels to one-hot\n",
        "        labels_onehot = eye_matrix[labels]\n",
        "        \n",
        "        # Determine if we should compute generator gradient\n",
        "        COMPUTE_GENERATOR_GRADIENT = (global_step % config['dfake_gen_update_ratio'] == 0)\n",
        "        \n",
        "        # ========== Generator Turn ==========\n",
        "        # Generate scaled noise\n",
        "        scaled_noise = torch.randn_like(images) * config['conditioning_sigma']\n",
        "        timestep_sigma = torch.ones(images.shape[0], device=device) * config['conditioning_sigma']\n",
        "        \n",
        "        # Random labels for generation\n",
        "        gen_labels = torch.randint(0, 10, (images.shape[0],), device=device)\n",
        "        gen_labels_onehot = eye_matrix[gen_labels]\n",
        "        \n",
        "        # Real training dict (for optional GAN loss)\n",
        "        real_train_dict = {\n",
        "            \"real_image\": images,\n",
        "            \"real_label\": labels_onehot\n",
        "        }\n",
        "        \n",
        "        # Forward pass through generator\n",
        "        generator_loss_dict, generator_log_dict = model(\n",
        "            scaled_noisy_image=scaled_noise,\n",
        "            timestep_sigma=timestep_sigma,\n",
        "            labels=gen_labels_onehot,\n",
        "            real_train_dict=real_train_dict if COMPUTE_GENERATOR_GRADIENT else None,\n",
        "            compute_generator_gradient=COMPUTE_GENERATOR_GRADIENT,\n",
        "            generator_turn=True,\n",
        "            guidance_turn=False\n",
        "        )\n",
        "        \n",
        "        # Update generator if needed\n",
        "        if COMPUTE_GENERATOR_GRADIENT:\n",
        "            generator_loss = generator_loss_dict[\"loss_dm\"] * config['dm_loss_weight']\n",
        "            \n",
        "            optimizer_generator.zero_grad()\n",
        "            generator_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                model.feedforward_model.parameters(),\n",
        "                config['max_grad_norm']\n",
        "            )\n",
        "            optimizer_generator.step()\n",
        "            optimizer_generator.zero_grad()\n",
        "            optimizer_guidance.zero_grad()\n",
        "            \n",
        "            epoch_loss_dm += generator_loss.item()\n",
        "        \n",
        "        # ========== Guidance Turn ==========\n",
        "        # Update guidance model (fake_unet)\n",
        "        guidance_loss_dict, guidance_log_dict = model(\n",
        "            scaled_noisy_image=None,  # Not used in guidance turn\n",
        "            timestep_sigma=None,  # Not used in guidance turn\n",
        "            labels=None,  # Not used in guidance turn\n",
        "            compute_generator_gradient=False,\n",
        "            generator_turn=False,\n",
        "            guidance_turn=True,\n",
        "            guidance_data_dict=generator_log_dict['guidance_data_dict']\n",
        "        )\n",
        "        \n",
        "        guidance_loss = guidance_loss_dict[\"loss_fake_mean\"]\n",
        "        \n",
        "        optimizer_guidance.zero_grad()\n",
        "        guidance_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            model.guidance_model.fake_unet.parameters(),\n",
        "            config['max_grad_norm']\n",
        "        )\n",
        "        optimizer_guidance.step()\n",
        "        optimizer_guidance.zero_grad()\n",
        "        optimizer_generator.zero_grad()\n",
        "        \n",
        "        epoch_loss_fake += guidance_loss.item()\n",
        "        \n",
        "        global_step += 1\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            \"loss_dm\": epoch_loss_dm / max(1, (batch_idx + 1) // config['dfake_gen_update_ratio']),\n",
        "            \"loss_fake\": epoch_loss_fake / (batch_idx + 1),\n",
        "        })\n",
        "        \n",
        "        # Save checkpoint periodically\n",
        "        if global_step % config['save_every'] == 0:\n",
        "            checkpoint_path = os.path.join(\n",
        "                config['output_dir'],\n",
        "                f\"dmd2_checkpoint_step_{global_step}.pt\"\n",
        "            )\n",
        "            torch.save({\n",
        "                'feedforward_model_state_dict': model.feedforward_model.state_dict(),\n",
        "                'guidance_fake_unet_state_dict': model.guidance_model.fake_unet.state_dict(),\n",
        "                'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
        "                'optimizer_guidance_state_dict': optimizer_guidance.state_dict(),\n",
        "                'step': global_step,\n",
        "                'epoch': epoch,\n",
        "            }, checkpoint_path)\n",
        "            print(f\"\\nSaved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "# Save final checkpoint\n",
        "final_checkpoint_path = os.path.join(config['output_dir'], \"dmd2_final.pt\")\n",
        "torch.save({\n",
        "    'feedforward_model_state_dict': model.feedforward_model.state_dict(),\n",
        "    'guidance_fake_unet_state_dict': model.guidance_model.fake_unet.state_dict(),\n",
        "    'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
        "    'optimizer_guidance_state_dict': optimizer_guidance.state_dict(),\n",
        "    'step': global_step,\n",
        "    'epoch': config['num_epochs'],\n",
        "}, final_checkpoint_path)\n",
        "print(f\"\\nSaved final checkpoint to {final_checkpoint_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
