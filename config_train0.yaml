# Shared training config example (works with both train_teacher.py and train_dmd2.py)
# - Each script will ignore unknown keys (useful to keep a single config file).
# - CLI flags override anything set here.

# ---- data / io ----
data_dir: ./data

# ---- teacher training (train_teacher.py) ----
output_dir: ./log/checkpoints/teacher
batch_size: 128
lr: 1.0e-4
num_train_timesteps: 1000
sigma_min: 0.002
sigma_max: 80.0
sigma_data: 0.5
rho: 7.0
max_grad_norm: 1.0
save_every: 5000

# Optional W&B block (train_teacher.py)
wandb:
  enabled: true
  project: minimal-dmd
  run_name: teacher-train0
  mode: online          # online|offline|disabled
  dir: ./log/wandb
  tags: ["teacher", "mnist", "train0"]
  log_every: 50
  watch: false
  log_images: true
  num_log_images: 32
  log_checkpoints: true
  # Periodic sampling from the teacher during training
  log_samples: true
  sample_every: 1000
  sample_num_images: 64
  sample_steps: 20
  sample_conditioning_sigma: 80.0

# ---- dmd2 training (train_dmd2.py) ----
# (train_teacher.py will ignore these keys)
output_dir: ./log/checkpoints/dmd2  # NOTE: override this when using this config for DMD2
teacher_checkpoint: ./log/checkpoints/teacher/teacher_final.pt
generator_lr: 2.0e-6
guidance_lr: 2.0e-6
step_number: 400000
min_step_percent: 0.02
max_step_percent: 0.98
conditioning_sigma: 80.0
dfake_gen_update_ratio: 10
dm_loss_weight: 1.0
# resume_from_checkpoint: null  # Optional: path to DMD2 checkpoint to resume from

# W&B for DMD2 (can reuse the same wandb: block above, or override here)
# The wandb: block above will be used by both scripts if present