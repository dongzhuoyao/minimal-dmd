# DMD2 training config (train_dmd2.py)
# Use this config file for training the distilled DMD2 model

# ---- data / io ----
data_dir: ./data
output_dir: ./log/checkpoints/dmd2

# ---- required: teacher checkpoint ----
teacher_checkpoint: ./log/checkpoints/teacher/teacher_checkpoint_step_10000.pt

# ---- training hyperparameters ----
batch_size: 128
generator_lr: 2.0e-6
guidance_lr: 2.0e-6
step_number: 100000
num_train_timesteps: 1000
sigma_min: 0.002
sigma_max: 80.0
sigma_data: 0.5
rho: 7.0
max_grad_norm: 1.0
save_every: 5000

# ---- DMD2-specific hyperparameters ----
min_step_percent: 0.02
max_step_percent: 0.98
conditioning_sigma: 80.0
dfake_gen_update_ratio: 10
dm_loss_weight: 1.0

# ---- checkpoint resuming (optional) ----
# resume_from_checkpoint: null  # Set to a checkpoint path to resume training

# ---- Weights & Biases logging (optional) ----
wandb:
  enabled: true
  project: minimal-dmd
  run_name: dmd2-train1
  mode: online          # online|offline|disabled
  dir: ./log/wandb
  tags: ["dmd2", "mnist", "train1"]
  log_every: 50
  watch: false
  log_images: true
  num_log_images: 32
  log_checkpoints: true
  # Periodic sampling from the distilled generator during training
  log_samples: true
  sample_every: 1000
  sample_num_images: 64
  # Note: DMD2 sampling doesn't use sample_steps (single forward pass)
